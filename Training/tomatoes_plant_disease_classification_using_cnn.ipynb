{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22ef40b6-9e68-4249-a4b5-b2304a1c1497",
   "metadata": {},
   "source": [
    "#Importing modules\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tensorflow.keras import models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras import models, layers\n",
    "from PIL import Image\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f09458da-50da-4d3e-b569-84a5d2595aaf",
   "metadata": {},
   "source": [
    "import tensorflow as tf; print(tf.__version__)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a99f2ba3-d7df-4368-9ac7-6361ed9b3153",
   "metadata": {},
   "source": [
    "#using tensforlow's dataset to download images into tf.data.dataset\n",
    "STD_IMG_SIZE= 256 #constant image size\n",
    "BATCH_SIZE= 32\n",
    "EPISODES= 20\n",
    "CHANNELS=3\n",
    "\n",
    "#storing images into dataset\n",
    "\n",
    "tomato_image_dataset= tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"dataset_images\", shuffle=True, image_size=(STD_IMG_SIZE, STD_IMG_SIZE), batch_size= BATCH_SIZE\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db00a56c",
   "metadata": {},
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define data augmentation parameters\n",
    "train_data_augmentation = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Create a data generator for training images\n",
    "train_data_generator = train_data_augmentation.flow_from_directory(\n",
    "    \"dataset/train\",\n",
    "    target_size=(STD_IMG_SIZE, STD_IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"sparse\"\n",
    ")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8a7a3f0",
   "metadata": {},
   "source": [
    "train_data_generator.class_indices"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b78251e0",
   "metadata": {},
   "source": [
    "class_names = list(train_data_generator.class_indices.keys())\n",
    "class_names"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6da6aad7",
   "metadata": {},
   "source": [
    "from PIL import Image\n",
    "!pip3 install Pillow==6.0.0"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a857f434",
   "metadata": {},
   "source": [
    "for image_batch, label_batch in train_data_generator:\n",
    "    print(image_batch.shape)\n",
    "    break"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a39caa2d",
   "metadata": {},
   "source": [
    "# Define data augmentation parameters for validation dataset\n",
    "validation_data_augmentation = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Create a data generator for validation dataset\n",
    "validation_data_generator = train_data_augmentation.flow_from_directory(\n",
    "    \"dataset/val\",\n",
    "    target_size=(STD_IMG_SIZE, STD_IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"sparse\"\n",
    ")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef60088c",
   "metadata": {},
   "source": [
    "# Define data augmentation parameters for test dataset\n",
    "test_data_augmentation = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Create a data generator for validation dataset\n",
    "test_data_generator = train_data_augmentation.flow_from_directory(\n",
    "    \"dataset/test\",\n",
    "    target_size=(STD_IMG_SIZE, STD_IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"sparse\"\n",
    ")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67962819",
   "metadata": {},
   "source": [
    "# Define the input shape for the model, including batch size (32), image dimensions (STD_IMG_SIZE x STD_IMG_SIZE), and color channels (3 for RGB).\n",
    "input_shape = (BATCHSTD_IMG_SIZE, STD_IMG_SIZE, CHANNELS) \n",
    "\n",
    "\n",
    "# Define the Sequential model with data augmentation and resizing/rescaling layers.\n",
    "model = models.Sequential([\n",
    "    layers.InputLayer(input_shape=input_shape),\n",
    "\n",
    "    # Add Convolutional layers with ReLU activation for feature extraction.\n",
    "    layers.Conv2D(32, kernel_size = (3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D((2, 2)),  # Apply MaxPooling for downsampling.\n",
    "\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),  # Additional Conv2D layer.\n",
    "    layers.MaxPooling2D((2, 2)),  # MaxPooling after the second Conv2D layer.\n",
    "\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),  # Another Conv2D layer.\n",
    "    layers.MaxPooling2D((2, 2)),  # MaxPooling after the third Conv2D layer.\n",
    "\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),  # Fourth Conv2D layer.\n",
    "    layers.MaxPooling2D((2, 2)),  # MaxPooling after the fourth Conv2D layer.\n",
    "\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),  # Fifth Conv2D layer.\n",
    "    layers.MaxPooling2D((2, 2)),  # MaxPooling after the fifth Conv2D layer.\n",
    "\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),  # Fifth Conv2D layer.\n",
    "    layers.MaxPooling2D((2, 2)),  # MaxPooling after the fifth Conv2D layer.\n",
    "\n",
    "    layers.Flatten(),  # Flatten the 2D feature maps into a 1D vector for Dense layers.\n",
    "    \n",
    "    layers.Dense(64, activation=\"relu\"),  # Dense layer with ReLU activation.\n",
    "    layers.Dense(3, activation=\"softmax\"),  # Final Dense layer for classification.\n",
    "])\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "186a86ae-3708-48a7-9c64-a2d7ddab9c8b",
   "metadata": {},
   "source": [
    "model.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2092e7a-6e6c-4cba-8d13-4f9ca999cf0a",
   "metadata": {},
   "source": [
    "#this prints the total number of batches that has been produced by tensorflow input pipeline\n",
    "# every batch is of size 32 as we have mentioned in global variable. Thus, 32 batches multiplied by len value equals to total no. of images\n",
    "len(tomato_image_dataset)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "785747e2-ca6b-480d-8120-0a7a3c2e2b0c",
   "metadata": {},
   "source": [
    "# Set figure size and spacing between subplots\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "# Displaying few images to check if the labels correspond correctly to the images\n",
    "for batch, labels in tomato_image_dataset.take(1):\n",
    "    for i in range(15):  # Displaying 15 different shuffled images\n",
    "        ax = plt.subplot(3, 5, i + 1)  # Adjust subplot layout to 3x5 grid\n",
    "        plt.imshow(batch[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plt.show() "
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "47b18fe9-0324-4d8f-a062-728fe9b1fd81",
   "metadata": {},
   "source": [
    "ðŸ‘‡\n",
    "model.compile sets up the training configuration for the model, including how it learns (optimizer), how it measures performance (loss function), and what metrics to track during training. Each parameter is carefully chosen to optimize model training and improve classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17a8f751-9c63-4059-859d-d3fe89a8fa62",
   "metadata": {},
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "592c90c1-b81b-4dec-84ea-76f7e843985c",
   "metadata": {},
   "source": [
    "# Train the model using model.fit()\n",
    "epoch_history = model.fit(\n",
    "    train_data_generator,\n",
    "    epochs=EPISODES,\n",
    "    steps_per_epoch=47,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1,\n",
    "    \n",
    "    validation_data=validation_data_generator\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df668403",
   "metadata": {},
   "source": [
    "scores = model.evaluate(test_data_generator)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6662967b",
   "metadata": {},
   "source": [
    "scores"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ccc2d58",
   "metadata": {},
   "source": [
    "clss= class_data_random.stop"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f50e324d",
   "metadata": {},
   "source": [
    "### Let's visualizae the accuracy and loss curves of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8529b621-839e-4d3d-8646-b61b19c9e5dd",
   "metadata": {},
   "source": [
    "\n",
    "# Set the style and color palette\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "# Handle infinite values in epoch_history\n",
    "epoch_history.history[\"accuracy\"] = np.where(np.isinf(epoch_history.history[\"accuracy\"]), np.nan, epoch_history.history[\"accuracy\"])\n",
    "epoch_history.history[\"val_accuracy\"] = np.where(np.isinf(epoch_history.history[\"val_accuracy\"]), np.nan, epoch_history.history[\"val_accuracy\"])\n",
    "epoch_history.history['loss'] = np.where(np.isinf(epoch_history.history['loss']), np.nan, epoch_history.history['loss'])\n",
    "epoch_history.history['val_loss'] = np.where(np.isinf(epoch_history.history['val_loss']), np.nan, epoch_history.history['val_loss'])\n",
    "\n",
    "# Create a figure for displaying graphs\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plotting Training and Validation Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.lineplot(x=range(len(epoch_history.history[\"accuracy\"])), y=epoch_history.history[\"accuracy\"], label='Training Accuracy')\n",
    "sns.lineplot(x=range(len(epoch_history.history[\"val_accuracy\"])), y=epoch_history.history[\"val_accuracy\"], label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Plotting Training and Validation Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.lineplot(x=range(len(epoch_history.history['loss'])), y=epoch_history.history['loss'], label='Training Loss')\n",
    "sns.lineplot(x=range(len(epoch_history.history['val_loss'])), y=epoch_history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "30a6c94c-20f0-4a2f-8679-a083abcfa1a7",
   "metadata": {},
   "source": [
    "ðŸ‘‡\n",
    "using expand_dims to add an extra dimension at index 0 creates a batch of one image, which is necessary when working with models that expect input data in batches, even if you're processing a single image. This ensures compatibility between the input data shape and the model's input requirements.\n",
    "\n",
    "Why It's Necessary:\n",
    "Machine learning models, especially deep learning models, are often designed to process data in batches for efficiency and parallelization.\n",
    "Even if you're working with a single image during inference (making predictions), the model expects input data in batch format.\n",
    "Adding this extra dimension ensures that the input data conforms to the expected batch format, allowing the model to process the image correct\n",
    "\n",
    "ly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28445dd4-9a36-4fa6-af56-235865184c25",
   "metadata": {},
   "source": [
    "def predict(trained_model, input_image):\n",
    "    # Convert the input image to an array and add a batch dimension\n",
    "    input_image_array = tf.expand_dims(tf.keras.preprocessing.image.img_to_array(input_image[i]), 0)\n",
    "    \n",
    "    # Make predictions using the trained model\n",
    "    predictions = trained_model.predict(input_image_array)\n",
    "\n",
    "    # Extract predicted class and confidence\n",
    "    predicted_class = class_names[np.argmax(predictions[0])]\n",
    "    confidence = round(100 * np.max(predictions[0]), 2)\n",
    "    \n",
    "    return predicted_class, confidence\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "021a1876-3644-4ee0-9061-a90b7297de06",
   "metadata": {},
   "source": [
    "## Running prediction on few images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09f5396a-fab3-4b88-b0ac-50f5522e5cdb",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "for images, labels in test_data_generator:\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i])\n",
    "        \n",
    "        predicted_class, confidence = predict(model, images[i])\n",
    "        actual_class = class_names[int(labels[i])] \n",
    "        \n",
    "        plt.title(f\"Actual: {actual_class},\\n Predicted: {predicted_class}.\\n Confidence: {confidence}%\")\n",
    "        \n",
    "        plt.axis(\"off\")\n",
    "    break"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6ee1c1fb-6b4e-4941-bf12-43f61d71b624",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d543a414-1053-4199-b307-6b57994e062f",
   "metadata": {},
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the directory to save models\n",
    "model_dir = \"../model_versions\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Find the maximum version number in the directory\n",
    "versions = [int(i.split('_')[1].split('.')[0]) for i in os.listdir(model_dir) if i.startswith('v_')]\n",
    "latest_version = max(versions + [0])\n",
    "new_version = latest_version + 1\n",
    "\n",
    "# Save the model in SavedModel format\n",
    "saved_model_dir = f\"{model_dir}/v_{new_version}\"\n",
    "tf.saved_model.save(model, saved_model_dir)\n",
    "\n",
    "# Optionally, save a copy of the model in .h5 format\n",
    "model.save(f\"{saved_model_dir}/model_v{new_version}.h5\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255c964c-84fb-41a0-bab7-8f016cdeb1bf",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
