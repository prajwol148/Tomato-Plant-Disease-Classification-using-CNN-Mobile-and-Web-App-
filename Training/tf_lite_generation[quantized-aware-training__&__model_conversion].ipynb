{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0ea181-c0e7-41ea-b658-7da2570ae34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3da4f99-94ac-44bf-b457-51da0cdd5e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ef40b6-9e68-4249-a4b5-b2304a1c1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "!pip install --upgrade tensorflow-model-optimization\n",
    "import tensorflow_model_optimization as tfmot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7929a33-be35-4dc1-972f-a493d3224520",
   "metadata": {},
   "source": [
    "## Converting the already trained saved .pb model to tensorflow lite (tf-lite) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2ea6ef-813e-4676-9948-b7aeaa6cda13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Path to the directory containing the SavedModel\n",
    "saved_model_dir = \"../model_versions/1\"\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(saved_model_dir):\n",
    "    print(f\"Error: Directory '{saved_model_dir}' does not exist.\")\n",
    "else:\n",
    "    # Convert the SavedModel to TensorFlow Lite format\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        tflite_model = converter.convert()\n",
    "\n",
    "        # Save the TensorFlow Lite model\n",
    "        output_dir = 'tf_lite_models'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        tflite_model_filename = 'converted_model.tflite'\n",
    "        tflite_model_path = os.path.join(output_dir, tflite_model_filename)\n",
    "        with open(tflite_model_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "\n",
    "        print(\"Model converted successfully and saved to:\", tflite_model_path)\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred during conversion:\", str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c91682-bdd9-42c0-8fc2-3e7c42db0445",
   "metadata": {},
   "source": [
    "## ----using Quantized Aware Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a99f2ba3-d7df-4368-9ac7-6361ed9b3153",
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_LR = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "IMAGE_SIZE = 256\n",
    "default_image_size = tuple((IMAGE_SIZE, IMAGE_SIZE))\n",
    "image_size = 0\n",
    "data_dir = \"../../dataset_images\"\"\n",
    "CHANNELS=3\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "186a86ae-3708-48a7-9c64-a2d7ddab9c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_partitions_tf(ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "    \n",
    "    ds_size = ds.cardinality().numpy()\n",
    "    \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle_size, seed=12)\n",
    "    \n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    \n",
    "    train_ds = ds.take(train_size)    \n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2092e7a-6e6c-4cba-8d13-4f9ca999cf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  seed=123,\n",
    "  image_size=default_image_size,\n",
    "  batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "\n",
    "train_ds, val_ds, test_ds = get_dataset_partitions_tf(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b859087-dee6-48f3-8ab5-299299a4c3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = dataset.class_names\n",
    "n_classes = len(class_names)\n",
    "print(n_classes, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f3ae118-1c8b-440f-9d78-7bdefb332c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09240a7e-ce7d-4b95-ac32-73d4652091b9",
   "metadata": {},
   "source": [
    "<CENTER><sub>\n",
    "<h4>Using train_test_split from scikit-learn:</h4>\n",
    "Especially when working with tabular data or using scikit-learn for machine learning tasks, there are built-in methods for splitting datasets into training, validation, and testing sets. The train_test_split function from scikit-learn is commonly used for this purpose.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'X' is your feature data and 'y' is your target labels\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12430dd-d933-47a6-a0ac-da458ffe5eb8",
   "metadata": {},
   "source": [
    "<CENTER><sub>\n",
    "<h4>Choosing Between the Methods:</h4>\n",
    "If we're working with structured/tabular data and traditional machine learning algorithms, scikit-learn's train_test_split method is often more convenient and suitable. This method is well-established and efficient for splitting structured data into training and testing sets.\n",
    "\n",
    "However, for complex data types such as images or for deep learning tasks, using a custom splitting function may be the better approach. Custom functions allow for more flexibility and control over how the dataset is divided, which is crucial for tasks like image classification where specific data preprocessing and augmentation may be needed.\n",
    "</sub></CENTER>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e72604a-ed10-400a-a11b-6e027cac7552",
   "metadata": {},
   "source": [
    "<CENTER><sub>\n",
    "<h4>Developing a Custom Dataset Distribution Function:</h4>\n",
    "Since the task is image classification, we are developing our own function to distribute training, testing, and validation datasets. This is because scikit-learn is best suited for structured/tabular data and may not handle complex data like images efficientlyset, testing_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca23a7ca-999d-4717-b0ff-2947c33b839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48561bc1-0b40-4fad-b690-9ec20161d277",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6989e006-009a-4379-be98-1b54dc311a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_and_rescale = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
    "  layers.experimental.preprocessing.Rescaling(1./255),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855d4d16-2f8f-434d-8ee1-220f087007c7",
   "metadata": {},
   "source": [
    "<CENTER><sub>\n",
    "<h4>Caching the dataset:</h4>\n",
    "Caching the dataset (training_data.cache()) helps speed up data loading by storing elements in memory or on disk, reducing the overhead of preprocessing during training epochs.\n",
    "\n",
    "<h4>Shuffling the dataset:</h4>\n",
    "Shuffling the dataset (training_data.shuffle(1000)) introduces randomness into the data, preventing the model from memorizing the order of samples and improving its generalization ability.\n",
    "\n",
    "<h4>Prefetching data batches:</h4>\n",
    "Prefetching data batches (training_data.prefetch()) allows for smoother training by fetching the next batch of data while the current batch is being processed, reducing idle time and improving GPU/CPU utilization.\n",
    "</sub></CENTER>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f8ef040-2921-478a-8f43-fd2ae072dcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525dce0b-35ae-4dc7-85a1-a5d71ca9ced7",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "a) Resizing the image to the standard size we defined earlier (256x256). Why are we doing it again, even though we did it before?\n",
    "=> This is because the subsequent layer we create will be used in our final model. When our model makes predictions, if it encounters an image that isn't 256x256, it will handle it appropriately.\n",
    "\n",
    "b) Dividing RGB values by 255 is a form of normalization that scales pixel values to a range between 0 and 1. This normalization aids in training machine learning models, particularly neural networks, by ensuring uniformity in input feature scales. It also helps prevent numerical instability and overflow/underflow issues during computations, promoting faster convergence and improved model performance.\n",
    "\n",
    "c) RandomFlip and RandomRotation is important for image classification because it introduces variations in the training data, such as random flips and rotations. These variations help the model generalize better by learning from a diverse set of examples, making it more robust to different orientations and perspectives in real-world images.y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b678f59-d434-48e7-8b29-fb4710189163",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "batch_input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "chanDim = -1\n",
    "if K.image_data_format() == \"channels_first\":\n",
    "    input_shape = (CHANNELS, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    batch_input_shape = (BATCH_SIZE, CHANNELS, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    chanDim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3dccdf0-adc0-41f3-b4f3-3e307916ced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "n_classes = 3\n",
    "\n",
    "model = models.Sequential([\n",
    "    resize_and_rescale,\n",
    "    data_augmentation,\n",
    "    layers.Conv2D(32, kernel_size = (3,3), activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(n_classes, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.build(input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d80653-972a-43dc-a415-50e033bcdbfb",
   "metadata": {},
   "source": [
    "## Implementing Data Augmentation to Train Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80345ae5-a46c-4a23-a4c9-db3241be6f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da2ceee5-6f7b-433f-96a2-977d1334826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "785747e2-ca6b-480d-8120-0a7a3c2e2b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=val_ds,\n",
    "    verbose=1,\n",
    "    epochs=EPOCHS,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b18fe9-0324-4d8f-a062-728fe9b1fd81",
   "metadata": {},
   "source": [
    "ðŸ‘‡\n",
    "model.compile sets up the training configuration for the model, including how it learns (optimizer), how it measures performance (loss function), and what metrics to track during training. Each parameter is carefully chosen to optimize model training and improve classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17a8f751-9c63-4059-859d-d3fe89a8fa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Calculating model accuracy\")\n",
    "scores = model.evaluate(test_ds)\n",
    "print(f\"Test Accuracy: {round(scores[1],4)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c90c1-b81b-4dec-84ea-76f7e843985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_quantization(layer):\n",
    "    if (\n",
    "        isinstance(layer, layers.Dense)\n",
    "        or isinstance(layer, layers.MaxPool2D)\n",
    "        or isinstance(layer, layers.Conv2D)\n",
    "    ):\n",
    "        return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
    "    return layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8529b621-839e-4d3d-8646-b61b19c9e5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_model = tf.keras.models.clone_model(\n",
    "    model,\n",
    "    clone_function=apply_quantization,\n",
    ")\n",
    "\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a6c94c-20f0-4a2f-8679-a083abcfa1a7",
   "metadata": {},
   "source": [
    "ðŸ‘‡\n",
    "using expand_dims to add an extra dimension at index 0 creates a batch of one image, which is necessary when working with models that expect input data in batches, even if you're processing a single image. This ensures compatibility between the input data shape and the model's input requirements.\n",
    "\n",
    "Why It's Necessary:\n",
    "Machine learning models, especially deep learning models, are often designed to process data in batches for efficiency and parallelization.\n",
    "Even if you're working with a single image during inference (making predictions), the model expects input data in batch format.\n",
    "Adding this extra dimension ensures that the input data conforms to the expected batch format, allowing the model to process the image correct\n",
    "\n",
    "ly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28445dd4-9a36-4fa6-af56-235865184c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_aware_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021a1876-3644-4ee0-9061-a90b7297de06",
   "metadata": {},
   "source": [
    "## Running prediction on few images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f5396a-fab3-4b88-b0ac-50f5522e5cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_history = quant_aware_model.fit(train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=val_ds,\n",
    "    verbose=1,\n",
    "    epochs=20,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee1c1fb-6b4e-4941-bf12-43f61d71b624",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d543a414-1053-4199-b307-6b57994e062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Calculating Quant Aware model accuracy\")\n",
    "scores = quant_aware_model.evaluate(test_ds)\n",
    "print(f\"Test Accuracy: {round(scores[1],4)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255c964c-84fb-41a0-bab7-8f016cdeb1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "quantized_tflite_model = converter.convert()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
