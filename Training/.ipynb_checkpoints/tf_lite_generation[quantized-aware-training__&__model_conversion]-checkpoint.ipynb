{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22ef40b6-9e68-4249-a4b5-b2304a1c1497",
   "metadata": {},
   "source": [
    "#Importing modules\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tensorflow.keras import models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a99f2ba3-d7df-4368-9ac7-6361ed9b3153",
   "metadata": {},
   "source": [
    "#using tensforlow's dataset to download images into tf.data.dataset\n",
    "STD_IMG_SIZE= 256 #constant image size\n",
    "BATCH_SIZE= 32\n",
    "EPISODES= 2\n",
    "CHANNELS=3\n",
    "\n",
    "#storing images into dataset\n",
    "\n",
    "tomato_image_dataset= tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"dataset_images\", shuffle=True, image_size=(STD_IMG_SIZE, STD_IMG_SIZE), batch_size= BATCH_SIZE\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "186a86ae-3708-48a7-9c64-a2d7ddab9c8b",
   "metadata": {},
   "source": [
    "#trying to figure out what are the classes that composes these 4500 files\n",
    "classes_dataset = tomato_image_dataset.class_names\n",
    "classes_dataset"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2092e7a-6e6c-4cba-8d13-4f9ca999cf0a",
   "metadata": {},
   "source": [
    "#this prints the total number of batches that has been produced by tensorflow input pipeline\n",
    "# every batch is of size 32 as we have mentioned in global variable. Thus, 32 batches multiplied by len value equals to total no. of images\n",
    "len(tomato_image_dataset)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b859087-dee6-48f3-8ab5-299299a4c3ae",
   "metadata": {},
   "source": [
    "#exploring the dataset\n",
    "for image_batch, label_batch in tomato_image_dataset.take(1):\n",
    "    print(image_batch[0].numpy())\n",
    "    print(image_batch.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f3ae118-1c8b-440f-9d78-7bdefb332c7e",
   "metadata": {},
   "source": [
    "# Set figure size and spacing between subplots\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "# Displaying few images to check if the labels correspond correctly to the images\n",
    "for batch, labels in tomato_image_dataset.take(1):\n",
    "    for i in range(15):  # Displaying 15 different shuffled images\n",
    "        ax = plt.subplot(3, 5, i + 1)  # Adjust subplot layout to 3x5 grid\n",
    "        plt.imshow(batch[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(classes_dataset[labels[i]])\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plt.show() "
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "09240a7e-ce7d-4b95-ac32-73d4652091b9",
   "metadata": {},
   "source": [
    "<CENTER><sub>\n",
    "<h4>Using train_test_split from scikit-learn:</h4>\n",
    "Especially when working with tabular data or using scikit-learn for machine learning tasks, there are built-in methods for splitting datasets into training, validation, and testing sets. The train_test_split function from scikit-learn is commonly used for this purpose.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'X' is your feature data and 'y' is your target labels\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12430dd-d933-47a6-a0ac-da458ffe5eb8",
   "metadata": {},
   "source": [
    "<CENTER><sub>\n",
    "<h4>Choosing Between the Methods:</h4>\n",
    "If we're working with structured/tabular data and traditional machine learning algorithms, scikit-learn's train_test_split method is often more convenient and suitable. This method is well-established and efficient for splitting structured data into training and testing sets.\n",
    "\n",
    "However, for complex data types such as images or for deep learning tasks, using a custom splitting function may be the better approach. Custom functions allow for more flexibility and control over how the dataset is divided, which is crucial for tasks like image classification where specific data preprocessing and augmentation may be needed.\n",
    "</sub></CENTER>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e72604a-ed10-400a-a11b-6e027cac7552",
   "metadata": {},
   "source": [
    "<CENTER><sub>\n",
    "<h4>Developing a Custom Dataset Distribution Function:</h4>\n",
    "Since the task is image classification, we are developing our own function to distribute training, testing, and validation datasets. This is because scikit-learn is best suited for structured/tabular data and may not handle complex data like images efficientlyset, testing_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca23a7ca-999d-4717-b0ff-2947c33b839b",
   "metadata": {},
   "source": [
    "def distribute_dataset_train_validate_test(dataset, shuffle=True, shuffle_size=10000, \n",
    "                                           train_split=0.8, test_split=0.1, validate_split=0.1):\n",
    "    dataset_size = len(dataset)\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(shuffle_size, seed=15)\n",
    "    \n",
    "    train_size = int(train_split * dataset_size)\n",
    "    validation_size = int(validate_split * dataset_size)\n",
    "    \n",
    "    training_dataset = dataset.take(train_size)\n",
    "    validation_dataset = dataset.skip(train_size).take(validation_size)\n",
    "    testing_dataset = dataset.skip(train_size + validation_size)\n",
    "\n",
    "    return training_dataset, validation_dataset, testing_dataset\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48561bc1-0b40-4fad-b690-9ec20161d277",
   "metadata": {},
   "source": [
    "training_data, validation_data, testing_data = distribute_dataset_train_validate_test(tomato_image_dataset)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6989e006-009a-4379-be98-1b54dc311a34",
   "metadata": {},
   "source": [
    "# checking if distribution has been succesfully carried out or not\n",
    "\n",
    "if(len(tomato_image_dataset) == len(training_data)+len(testing_data)+len(validation_data) ):\n",
    "    print(\"True\")\n",
    "else:\n",
    "    print(\"False\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "855d4d16-2f8f-434d-8ee1-220f087007c7",
   "metadata": {},
   "source": [
    "<CENTER><sub>\n",
    "<h4>Caching the dataset:</h4>\n",
    "Caching the dataset (training_data.cache()) helps speed up data loading by storing elements in memory or on disk, reducing the overhead of preprocessing during training epochs.\n",
    "\n",
    "<h4>Shuffling the dataset:</h4>\n",
    "Shuffling the dataset (training_data.shuffle(1000)) introduces randomness into the data, preventing the model from memorizing the order of samples and improving its generalization ability.\n",
    "\n",
    "<h4>Prefetching data batches:</h4>\n",
    "Prefetching data batches (training_data.prefetch()) allows for smoother training by fetching the next batch of data while the current batch is being processed, reducing idle time and improving GPU/CPU utilization.\n",
    "</sub></CENTER>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f8ef040-2921-478a-8f43-fd2ae072dcc1",
   "metadata": {},
   "source": [
    "# In terms of sequence:\n",
    "\n",
    "# Caching (cache()) happens first, storing the dataset in memory or on disk.\n",
    "# Shuffling (shuffle()) occurs after caching, ensuring that the cached data is shuffled.\n",
    "# Prefetching (prefetch()) happens last, enabling concurrent data loading and model training.\n",
    "\n",
    "training_data = training_data.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "validation_data = validation_data.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "testing_data = testing_data.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "525dce0b-35ae-4dc7-85a1-a5d71ca9ced7",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "a) Resizing the image to the standard size we defined earlier (256x256). Why are we doing it again, even though we did it before?\n",
    "=> This is because the subsequent layer we create will be used in our final model. When our model makes predictions, if it encounters an image that isn't 256x256, it will handle it appropriately.\n",
    "\n",
    "b) Dividing RGB values by 255 is a form of normalization that scales pixel values to a range between 0 and 1. This normalization aids in training machine learning models, particularly neural networks, by ensuring uniformity in input feature scales. It also helps prevent numerical instability and overflow/underflow issues during computations, promoting faster convergence and improved model performance.\n",
    "\n",
    "c) RandomFlip and RandomRotation is important for image classification because it introduces variations in the training data, such as random flips and rotations. These variations help the model generalize better by learning from a diverse set of examples, making it more robust to different orientations and perspectives in real-world images.y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b678f59-d434-48e7-8b29-fb4710189163",
   "metadata": {},
   "source": [
    "resize_and_rescale = tf.keras.Sequential([\n",
    "    layers.Resizing(STD_IMG_SIZE, STD_IMG_SIZE),\n",
    "    layers.Rescaling(1.0 / 255)\n",
    "])\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3dccdf0-adc0-41f3-b4f3-3e307916ced2",
   "metadata": {},
   "source": [
    "data_augmentation= tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.RandomRotation(0.2),\n",
    "])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "43d80653-972a-43dc-a415-50e033bcdbfb",
   "metadata": {},
   "source": [
    "## Implementing Data Augmentation to Train Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80345ae5-a46c-4a23-a4c9-db3241be6f04",
   "metadata": {},
   "source": [
    "training_data = training_data.map(\n",
    "    lambda x, y: (data_augmentation(x, training=True), y)\n",
    ").prefetch(buffer_size=tf.data.AUTOTUNE)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da2ceee5-6f7b-433f-96a2-977d1334826d",
   "metadata": {},
   "source": [
    "# Define the input shape for the model, including batch size (32), image dimensions (STD_IMG_SIZE x STD_IMG_SIZE), and color channels (3 for RGB).\n",
    "input_shape = (BATCH_SIZE, STD_IMG_SIZE, STD_IMG_SIZE, CHANNELS) \n",
    "\n",
    "# Define the Sequential model with data augmentation and resizing/rescaling layers.\n",
    "model = models.Sequential([\n",
    "    resize_and_rescale,    # Apply data augmentation techniques for image variations.\n",
    "\n",
    "    # Add Convolutional layers with ReLU activation for feature extraction.\n",
    "    layers.Conv2D(32, kernel_size = (3, 3), activation=\"relu\", input_shape=input_shape),\n",
    "    layers.MaxPooling2D((2, 2)),  # Apply MaxPooling for downsampling.\n",
    "\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),  # Additional Conv2D layer.\n",
    "    layers.MaxPooling2D((2, 2)),  # MaxPooling after the second Conv2D layer.\n",
    "\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),  # Another Conv2D layer.\n",
    "    layers.MaxPooling2D((2, 2)),  # MaxPooling after the third Conv2D layer.\n",
    "\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),  # Fourth Conv2D layer.\n",
    "    layers.MaxPooling2D((2, 2)),  # MaxPooling after the fourth Conv2D layer.\n",
    "\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),  # Fifth Conv2D layer.\n",
    "    layers.MaxPooling2D((2, 2)),  # MaxPooling after the fifth Conv2D layer.\n",
    "\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),  # Fifth Conv2D layer.\n",
    "    layers.MaxPooling2D((2, 2)),  # MaxPooling after the fifth Conv2D layer.\n",
    "\n",
    "    layers.Flatten(),  # Flatten the 2D feature maps into a 1D vector for Dense layers.\n",
    "    \n",
    "    layers.Dense(64, activation=\"relu\"),  # Dense layer with ReLU activation.\n",
    "    layers.Dense(3, activation=\"softmax\"),  # Final Dense layer for classification.\n",
    "])\n",
    "model.build(input_shape=input_shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "785747e2-ca6b-480d-8120-0a7a3c2e2b0c",
   "metadata": {},
   "source": [
    "model.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "47b18fe9-0324-4d8f-a062-728fe9b1fd81",
   "metadata": {},
   "source": [
    "ðŸ‘‡\n",
    "model.compile sets up the training configuration for the model, including how it learns (optimizer), how it measures performance (loss function), and what metrics to track during training. Each parameter is carefully chosen to optimize model training and improve classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17a8f751-9c63-4059-859d-d3fe89a8fa62",
   "metadata": {},
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c90c1-b81b-4dec-84ea-76f7e843985c",
   "metadata": {},
   "source": [
    "# Train the model using model.fit()\n",
    "epoch_history = model.fit(\n",
    "    training_data,\n",
    "    epochs=EPISODES,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1,\n",
    "    validation_data=validation_data\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8529b621-839e-4d3d-8646-b61b19c9e5dd",
   "metadata": {},
   "source": [
    "\n",
    "# Set the style and color palette\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "# Handle infinite values in epoch_history\n",
    "epoch_history.history[\"accuracy\"] = np.where(np.isinf(epoch_history.history[\"accuracy\"]), np.nan, epoch_history.history[\"accuracy\"])\n",
    "epoch_history.history[\"val_accuracy\"] = np.where(np.isinf(epoch_history.history[\"val_accuracy\"]), np.nan, epoch_history.history[\"val_accuracy\"])\n",
    "epoch_history.history['loss'] = np.where(np.isinf(epoch_history.history['loss']), np.nan, epoch_history.history['loss'])\n",
    "epoch_history.history['val_loss'] = np.where(np.isinf(epoch_history.history['val_loss']), np.nan, epoch_history.history['val_loss'])\n",
    "\n",
    "# Create a figure for displaying graphs\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plotting Training and Validation Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.lineplot(x=range(len(epoch_history.history[\"accuracy\"])), y=epoch_history.history[\"accuracy\"], label='Training Accuracy')\n",
    "sns.lineplot(x=range(len(epoch_history.history[\"val_accuracy\"])), y=epoch_history.history[\"val_accuracy\"], label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Plotting Training and Validation Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.lineplot(x=range(len(epoch_history.history['loss'])), y=epoch_history.history['loss'], label='Training Loss')\n",
    "sns.lineplot(x=range(len(epoch_history.history['val_loss'])), y=epoch_history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "30a6c94c-20f0-4a2f-8679-a083abcfa1a7",
   "metadata": {},
   "source": [
    "ðŸ‘‡\n",
    "using expand_dims to add an extra dimension at index 0 creates a batch of one image, which is necessary when working with models that expect input data in batches, even if you're processing a single image. This ensures compatibility between the input data shape and the model's input requirements.\n",
    "\n",
    "Why It's Necessary:\n",
    "Machine learning models, especially deep learning models, are often designed to process data in batches for efficiency and parallelization.\n",
    "Even if you're working with a single image during inference (making predictions), the model expects input data in batch format.\n",
    "Adding this extra dimension ensures that the input data conforms to the expected batch format, allowing the model to process the image correct\n",
    "\n",
    "ly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28445dd4-9a36-4fa6-af56-235865184c25",
   "metadata": {},
   "source": [
    "def predict(trained_model, input_image):\n",
    "    # Convert the input image to an array and add a batch dimension\n",
    "    input_image_array = tf.expand_dims(tf.keras.preprocessing.image.img_to_array(input_image), 0)\n",
    "    \n",
    "    # Make predictions using the trained model\n",
    "    predictions = trained_model.predict(input_image_array)\n",
    "\n",
    "    # Extract predicted class and confidence\n",
    "    predicted_class = classes_dataset[np.argmax(predictions[0])]\n",
    "    confidence = round(100 * np.max(predictions[0]), 2)\n",
    "    \n",
    "    return predicted_class, confidence\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "021a1876-3644-4ee0-9061-a90b7297de06",
   "metadata": {},
   "source": [
    "## Running prediction on few images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f5396a-fab3-4b88-b0ac-50f5522e5cdb",
   "metadata": {},
   "source": [
    "# Create a figure for displaying images\n",
    "plt.figure(figsize=(18,18))\n",
    "\n",
    "# Iterate through the first batch of images and labels in the test dataset\n",
    "for batch_images, batch_labels in testing_data.take(1):\n",
    "    # Display only 15 images\n",
    "    for i in range(15):\n",
    "        # Create a subplot for each image\n",
    "        ax = plt.subplot(3,5, i + 1)\n",
    "        \n",
    "        # Display the image\n",
    "        plt.imshow(batch_images[i].numpy().astype(\"uint8\"))\n",
    "        \n",
    "        # Get the predicted class and confidence\n",
    "        predicted_class, confidence = predict(model, batch_images[i].numpy())\n",
    "        \n",
    "        # Get the actual class from class_names using labels\n",
    "        actual_class = classes_dataset[batch_labels[i]] \n",
    "        \n",
    "        # Set the title with actual class, predicted class, and confidence\n",
    "        plt.title(f\"Actual: {actual_class},\\nPredicted: {predicted_class}.\\nConfidence: {confidence}%\")\n",
    "        \n",
    "        # Turn off axis for cleaner display\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6ee1c1fb-6b4e-4941-bf12-43f61d71b624",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d543a414-1053-4199-b307-6b57994e062f",
   "metadata": {},
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the directory to save models\n",
    "model_dir = \"../model_versions\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Find the maximum version number in the directory\n",
    "versions = [int(i.split('_')[1].split('.')[0]) for i in os.listdir(model_dir) if i.startswith('v_')]\n",
    "latest_version = max(versions + [0])\n",
    "new_version = latest_version + 1\n",
    "\n",
    "# Save the model in SavedModel format\n",
    "saved_model_dir = f\"{model_dir}/v_{new_version}\"\n",
    "tf.saved_model.save(model, saved_model_dir)\n",
    "\n",
    "# Optionally, save a copy of the model in .h5 format\n",
    "model.save(f\"{saved_model_dir}/model_v{new_version}.h5\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255c964c-84fb-41a0-bab7-8f016cdeb1bf",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
